{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 9\n",
        "\n",
        "This notebook covers three parts: an expansion on scikit-learn, Scipy for statistics, and Scipy for solving differential equations.\n",
        "\n",
        "## 1. More on scikit-learn\n",
        "\n",
        "We'll discuss another dimensionality reduction option - independent component analysis ([ICA](https://en.wikipedia.org/wiki/Independent_component_analysis)). It assumes that observed data is a sum of independent components, and then tries to learn how it is mixed to be able to un-mix the data into original sources. In practical terms, you can think of it as trying to separate multiple sound sources from a single microphone.\n",
        "\n",
        "### Signal generation\n",
        "\n",
        "First, let's generate some data:\n",
        "> 1. Create time array: an array of 2000 elements ranging from 0 to 2. Let's say it represents 2 seconds. Hint: there is a useful Numpy function for that!\n",
        "> 2. Create a sinusoidal signal with a frequency of 2 Hz and amplitude of 2 a.u. Hint: use [`np.sin()`](https://numpy.org/doc/stable/reference/generated/numpy.sin.html).\n",
        "> 3. Create a second sinusoidal signal with a frequency of 5 Hz, amplitude of 0.5 a.u., and phase of 0.1.\n",
        "> 4. Create any square signal using a combination of [`np.sign()`](https://numpy.org/doc/stable/reference/generated/numpy.sign.html) and a sinusoidal signal with a frequency of 3 Hz.\n",
        "> 5. Check how all of these signals look like by plotting them in a single plot with three subplots."
      ],
      "metadata": {
        "id": "6_SkxjzvLtZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "g5UfMc1wOpvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time = ...\n",
        "signal_sin2 = ...\n",
        "signal_sin5 = ...\n",
        "signal_square = ..."
      ],
      "metadata": {
        "id": "GxJ_SJj9Sn6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
        "..."
      ],
      "metadata": {
        "id": "gun_cH-eRz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add some noise and mix these signals:\n",
        "\n",
        "1. Join (stack) all three signals into a single array named `S`. Transform it so the shape is `(2000, 3)`.\n",
        "1. Create a sequence of gaussian noise (random floats taken from [normal](https://numpy.org/doc/2.0/reference/random/generated/numpy.random.normal.html) distribution), scale it to be smaller than initial signals (e.g multiply by 0.1), and add it to `S` (perform addition operation).\n",
        "1. Divide `S` by its standard deviation which should be calculated independently for each signal (so there should be 3 values).\n",
        "1. The mixing matrix `A` is already defined in code cell. Use it to get our mixed signal `X` by multiplying `S` by `A`. Hint: use matrix multiplication instead of element-wise multiplication!\n",
        "\n",
        "The mixing matrix `A` is what the model is trying to learn. This matrix tells you what the proportions of each original signal are, and thus can be used to recover original signals (separate them)."
      ],
      "metadata": {
        "id": "Snt-4tZPVNyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]]).T\n"
      ],
      "metadata": {
        "id": "L0BLZsw0X38k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the mixed signals.\n",
        "\n",
        "Try changing the mixing matrix. Can you make one of the mixed signals equal to one of the originals?"
      ],
      "metadata": {
        "id": "pbFvANb5ZHNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
        "..."
      ],
      "metadata": {
        "id": "RrMw4gaKZz0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ICA"
      ],
      "metadata": {
        "id": "m7tQkS1wBBUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it's time to try to recover our signals using ICA. In scikit-learn, it can be accessed as `FastICA`, and the estimated mixing matrix can be printed by `.mixing_` after fitting.\n",
        "> 1. Create a `FastICA` object with `n_components=3`, as we are trying to recover three components.\n",
        "> 2. Fit the ICA on `X`. Print the estimated mixing matrix. Is it similar to the original?\n",
        "> 3. Transform the mixed signals to get the predicted original components, and plot them over the real original signals. How similar are the predicted signals to the original ones? Did their order change? Does it changes again if you rerun ICA again?"
      ],
      "metadata": {
        "id": "Dk5udyCea6JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "ica = ..."
      ],
      "metadata": {
        "id": "ybiUlzqaaxzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Statistics with Scipy\n",
        "\n",
        "Scipy stands for Scientific Python, which is quite self-explanatory. It extends Numpy to include more functions for scientific computing, including statistics, signal processing, image processing, integration, linear algebra and more. This makes it impossible to cover everything, but let's look at some specific examples."
      ],
      "metadata": {
        "id": "1ly4OSwrdQrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stats\n",
        "\n",
        "Most commonly used statistical tests are available in `scipy.stats`. Let's import some data and run some statistical tests.\n",
        "\n",
        "- If you don't have kagglehub and don't want to install it, use this link: https://www.kaggle.com/api/v1/datasets/download/uciml/biomechanical-features-of-orthopedic-patients?dataset_version_number=1"
      ],
      "metadata": {
        "id": "JUJr5_Dvte9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"uciml/biomechanical-features-of-orthopedic-patients\")\n",
        "data_files = os.listdir(path)\n",
        "print('Downloaded files:', data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCeiIM1buunK",
        "outputId": "4fc84590-8937-4898-ea16-c39815d26a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded files: ['column_2C_weka.csv', 'column_3C_weka.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Load and explore the first file (2C) using pandas. You can get the path using `os.path.join(path, data_files[0])`.\n",
        "\n",
        "This should be a standard procedure for any datafiles: check its data types and basic statistics with two relevant Pandas functions."
      ],
      "metadata": {
        "id": "EW2givB3vzjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = ...\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JQYelbyhv7a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8kiue3s8EbTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a `class` column, which we will use for **getting data groups**.\n",
        "> Use the seaborn pairplot to plot all features. Set the hue parameter to `class` to have differently colored groups."
      ],
      "metadata": {
        "id": "auk4y7rzxIgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "..."
      ],
      "metadata": {
        "id": "YcCVTZhCxi3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Separate `df` into two groups: one group has only `class=0`, while another only has `class=1`."
      ],
      "metadata": {
        "id": "6vQfgxHfHisV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group0 = ...\n",
        "group1 = ..."
      ],
      "metadata": {
        "id": "JVGSGPinHsO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first sight, there does seem to be some difference between groups.\n",
        "\n",
        "Let's compare the lumbar lordosis angle between the groups. First, we need to check whether the data is normally distributed, because that would mean we can use parametric statistical methods.\n",
        "> Use the [`normaltest()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) function from Scipy to check whether the distribution of lumbar lordosis angle is normal in each group.  \n",
        "\n",
        "- As written on Scipy page: the `normaltest()` function tests the null hypothesis that a sample comes from a normal distribution. If the p-value is “small”, this may be taken as evidence against the null hypothesis, meaning that the values were not drawn from a normal distribution. Hovewer, keep in mind that the inverse is not true; that is, the test is not used to provide evidence for the null hypothesis."
      ],
      "metadata": {
        "id": "vv7n0qrrx2eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "..."
      ],
      "metadata": {
        "id": "Pf1oIP9Dx0Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since one of the distributions is not normal, a non-parametric test is more suitable in this case. An alternative of the independent sample t-test is the Mann-Whitney U test.\n",
        "> Separate groups into two\n",
        "> Perform the Mann-Whitney U test by using [`mannwhitneyu()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html). Is there a significant difference between groups?"
      ],
      "metadata": {
        "id": "YhTAxWGty-EC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKzWC1C_5ffD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Integration with Scipy\n",
        "\n",
        "Sometimes, we're interested how things change in time. This is described by differential equations. For example, how a population changes can be approximated as:\n",
        "\n",
        "$dP/dt = kP(1-P/L)$,\n",
        "\n",
        "- $P$: Population size\n",
        "- $L$: Limiting capacity (how many resources are available, ~ max population)\n",
        "- $k$: Constant of proportionality (birth rate/death rate)\n",
        "\n",
        "This is an example of an ordinary differential equation, which depends only on a single independent variable. Some of them have defined mathematical solutions, and others can have multiple or none, but finding out those solutions can be time-consuming, and thus done automatically.\n",
        "\n",
        "In Scipy, it can be done by using the [`odeint()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html) function. This function requires the time series, initial condition, $dy/dt$ function, and any optional arguments. Let's calculate it for the population equation.\n",
        "\n",
        "> 1. Write the population equation as a function `population_eq()`. It should take `P`, `t`, `k` and `L` as parameters, and return the `dpdt` expression.\n",
        "> 2. Create 1000 time points from 0 to 100. Set the initial conditions: `P0 = 200`, `k = 0.5`, `L = 100`.\n",
        "> 3. Solve the equation using `scipy.odeint(population_eq, P0, t, args=())`.\n",
        "> 4. Plot the solution.\n",
        "\n",
        "Play around with initial parameters. Can you get to a stable population size?"
      ],
      "metadata": {
        "id": "eN0cFT9-6cdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pop_eq(P, t, k, L):\n",
        "  dpdt = ...\n",
        "  return dpdt"
      ],
      "metadata": {
        "id": "2EFk2rRfN-pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time = ...\n"
      ],
      "metadata": {
        "id": "AUXVaL5uOjwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}